{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1co9L0f3cl6TV8YCyCmOdDjJmMEJKqMuR",
      "authorship_tag": "ABX9TyON6ehX0hfiuGk5x7UV8+no",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rumman-adnan/Assignment-Sensors-Data/blob/main/sensors_data_analytics_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4aRQ04_85VmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1934c955-07d3-461d-cd5c-59b37183a5aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuUnVL_h8AFI",
        "outputId": "0454640d-8768-47cd-baa7-a6c4fb55d540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n"
      ],
      "metadata": {
        "id": "dzEGkJVu9NvQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "# currently dont use this\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Python work/sim_data/CSV data/training_data.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Python work/sim_data/CSV data/testing_data.csv')\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n"
      ],
      "metadata": {
        "id": "7GZWki5XERCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefe926e-b348-4e77-cb1f-07e0658e2e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 1002)\n",
            "(10000, 1002)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load('/content/drive/MyDrive/Python work/sim_data/simu_20000_0.1_90_140_train.npy')\n",
        "test_data = np.load('/content/drive/MyDrive/Python work/sim_data/simu_10000_0.1_141_178_test.npy')\n",
        "\n",
        "print(\"The shape of trained data is: \",train_data.shape)\n",
        "print(\"The shape of tested data is: \",test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy_WJs1Ujo2u",
        "outputId": "cf53db3a-c958-48d4-ca44-e4581df1d5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of trained data is:  (20000, 1006)\n",
            "The shape of tested data is:  (10000, 1006)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features and target variables from the training dataset\n",
        "train_sensor_data = train_data[:, :1000]\n",
        "train_ID = train_data[:, 1000]\n",
        "train_time = train_data[:, 1001]\n",
        "train_H = train_data[:, 1002]\n",
        "train_R = train_data[:, 1003]\n",
        "train_S = train_data[:, 1004]\n",
        "train_D = train_data[:, 1005]\n",
        "\n",
        "# Extract features and target variables from the test dataset\n",
        "test_sensor_data = test_data[:, :1000]\n",
        "test_ID = test_data[:, 1000]\n",
        "test_time = test_data[:, 1001]\n",
        "test_H = test_data[:, 1002]\n",
        "test_R = test_data[:, 1003]\n",
        "test_S = test_data[:, 1004]\n",
        "test_D = test_data[:, 1005]\n",
        "\n",
        "# Display the shape of the extracted arrays to confirm extraction\n",
        "train_sensor_data.shape, train_S.shape, train_D.shape, test_sensor_data.shape, test_S.shape, test_D.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBKNuOQQiSn9",
        "outputId": "fbcda375-cc7f-4461-81c3-4fd381ae4b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000, 1000), (20000,), (20000,), (10000, 1000), (10000,), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Reshape the data to fit the input requirements of a 1D CNN model\n",
        "# The shape should be (num_samples, num_time_steps, num_features)\n",
        "train_sensor_data_cnn = train_sensor_data.reshape(train_sensor_data.shape[0], train_sensor_data.shape[1], 1)\n",
        "test_sensor_data_cnn = test_sensor_data.reshape(test_sensor_data.shape[0], test_sensor_data.shape[1], 1)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "train_sensor_data_cnn = scaler.fit_transform(train_sensor_data_cnn.reshape(-1, 1)).reshape(train_sensor_data_cnn.shape)\n",
        "test_sensor_data_cnn = scaler.transform(test_sensor_data_cnn.reshape(-1, 1)).reshape(test_sensor_data_cnn.shape)\n",
        "\n",
        "# Show the shape of the reshaped and normalized data\n",
        "train_sensor_data_cnn.shape, test_sensor_data_cnn.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMveGehGkEM9",
        "outputId": "a708e3c4-8067-424c-ea62-5ba94ddc8f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000, 1000, 1), (10000, 1000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN model architecture\n",
        "def build_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Convolutional layers\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=256, kernel_size=5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(2))  # Two output nodes to predict both systolic and diastolic BP\n",
        "\n",
        "    # Compile the model\n",
        "    # model.compile(optimizer=Adam(), loss='mse')\n",
        "    model.compile(optimizer='adam', loss='mse', epochs = 50, verbose = True,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = build_cnn_model((train_sensor_data_cnn.shape[1], 1))\n",
        "model.fit(train_sensor_data_cnn.shape[1], epochs=2, steps_per_epoch=10)\n",
        "# Display the model architecture\n",
        "cnn_model.summary()\n"
      ],
      "metadata": {
        "id": "zyCTkncEkLE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = cnn_model.predict(test_sensor_data_cnn)\n",
        "\n",
        "# Separate the predictions for systolic and diastolic BP\n",
        "pred_S, pred_D = predictions[:, 0], predictions[:, 1]\n",
        "\n",
        "# Compute the MAE for systolic and diastolic BP\n",
        "mae_S = mean_absolute_error(test_S, pred_S)\n",
        "mae_D = mean_absolute_error(test_D, pred_D)\n",
        "\n",
        "print(f\"Mean Absolute Error for Systolic BP: {mae_S}\")\n",
        "print(f\"Mean Absolute Error for Diastolic BP: {mae_D}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DEE0_OlkrYd",
        "outputId": "a631c55f-f72b-4a01-ff08-ddc8227d6b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step\n",
            "Mean Absolute Error for Systolic BP: 159.4602992392715\n",
            "Mean Absolute Error for Diastolic BP: 79.91103181238296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply CNN with Numpy arrays\n",
        "- Read numpy arrays instead of CSV data and apply algorithms on it\n",
        "- large epochs and paramters tunning till we acheive MAE 3\n"
      ],
      "metadata": {
        "id": "Q14YvVD3U_ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the numpy files\n",
        "test_data = np.load('/content/drive/MyDrive/Python work/sim_data/simu_10000_0.1_141_178_test.npy')\n",
        "train_data = np.load('/content/drive/MyDrive/Python work/sim_data/simu_20000_0.1_90_140_train.npy')\n",
        "\n",
        "# Get the shape of the data arrays\n",
        "test_shape = test_data.shape\n",
        "train_shape = train_data.shape\n",
        "\n",
        "test_shape, train_shape, test_data[:3], train_data[:3]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2BPGxGcXcAm",
        "outputId": "1c9b02b2-2436-40ff-8542-bddf1b8f1a4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 1006),\n",
              " (20000, 1006),\n",
              " array([[-7.14960675e-08, -3.61296976e-08,  3.41059619e-07, ...,\n",
              "          1.10000000e+01,  1.72000000e+02,  8.30000000e+01],\n",
              "        [ 2.77480001e-08,  2.82283796e-08,  2.99275705e-08, ...,\n",
              "          1.30000000e+01,  1.73000000e+02,  7.90000000e+01],\n",
              "        [ 1.00426931e-07,  1.00875737e-07,  3.30280515e-07, ...,\n",
              "          2.10000000e+01,  1.46000000e+02,  7.30000000e+01]]),\n",
              " array([[-2.45845714e-07, -2.06162897e-07,  1.56348382e-06, ...,\n",
              "          1.90000000e+01,  9.10000000e+01,  9.50000000e+01],\n",
              "        [ 2.70396897e-07,  4.12740460e-07,  2.72092277e-06, ...,\n",
              "          1.60000000e+01,  1.01000000e+02,  6.10000000e+01],\n",
              "        [ 1.97552550e-07,  5.92946672e-07,  4.53845770e-06, ...,\n",
              "          1.00000000e+01,  1.31000000e+02,  8.10000000e+01]]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into features and targets\n",
        "\n",
        "# Extracting sensor data (features)\n",
        "X_train = train_data[:, :1000]\n",
        "X_test = test_data[:, :1000]\n",
        "\n",
        "# Extracting target variables for Systolic (S) and Diastolic (D) blood pressure only\n",
        "Y_train_SD = train_data[:, -2:]\n",
        "Y_test_SD = test_data[:, -2:]\n",
        "\n",
        "# Names of the target variables for reference\n",
        "target_names_SD = [\"Systolic BP\", \"Diastolic BP\"]\n",
        "\n",
        "Y_train_SD.shape, Y_test_SD.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKOykbU6YCho",
        "outputId": "7902ff35-f72a-4bae-f3f0-3f9a6cfe2438"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000, 2), (10000, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple CNN,\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Reshaping data to be suitable for CNN (batch_size, steps, input_dim)\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Model Creation\n",
        "model = Sequential()\n",
        "\n",
        "# Adding Convolutional layers\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten and add Dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(2))  # Two output units for S and D\n",
        "\n",
        "# Compiling the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss=MeanSquaredError())\n",
        "\n",
        "# Train the model over several epochs\n",
        "epochs = 10\n",
        "history = model.fit(X_train_reshaped, Y_train_SD, validation_split=0.2, epochs=epochs, batch_size=32)\n",
        "\n",
        "# Evaluate the model's performance on the test set\n",
        "test_loss = model.evaluate(X_test_reshaped, Y_test_SD)\n",
        "\n",
        "# Predicting S and D from the test set\n",
        "predictions = model.predict(X_test_reshaped)\n",
        "\n",
        "# Calculating the MAE score\n",
        "mae_scores = mean_absolute_error(Y_test_SD, predictions, multioutput='raw_values')\n",
        "\n",
        "mae_scores\n"
      ],
      "metadata": {
        "id": "JiTclsf9kfCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Reshaping data to be suitable for CNN (batch_size, steps, input_dim)\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Model Creation\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(2))\n",
        "\n",
        "# Define optimizer and loss\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Custom training loop\n",
        "epochs = 500  # Maximum epochs\n",
        "batch_size = 32\n",
        "desired_mae = 3\n",
        "steps_per_epoch = len(X_train_reshaped) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Create a batch of data\n",
        "        X_batch = X_train_reshaped[step*batch_size:(step+1)*batch_size]\n",
        "        Y_batch = Y_train_SD[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(X_batch, training=True)\n",
        "            loss_value = loss_fn(Y_batch, predictions)\n",
        "\n",
        "        # Backward pass\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Evaluate after each epoch\n",
        "    predictions = model(X_test_reshaped)\n",
        "    mae_scores = mean_absolute_error(Y_test_SD, predictions, multioutput='raw_values')\n",
        "    print(\"MAE after epoch {}: {}\".format(epoch+1, mae_scores))\n",
        "\n",
        "    # If desired MAE is achieved, stop training\n",
        "    if mae_scores[0] <= desired_mae and mae_scores[1] <= desired_mae:\n",
        "        print(\"Desired MAE achieved. Stopping training.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "-s_U49NAQ6tJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23ab4e05-4488-416a-e38b-d1810d3406ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7d887f918d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7d887f918d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE after epoch 1: [41.630964   10.49081807]\n",
            "\n",
            "Epoch 2/500\n",
            "MAE after epoch 2: [43.12377963 10.26475824]\n",
            "\n",
            "Epoch 3/500\n",
            "MAE after epoch 3: [44.26942156 10.22227547]\n",
            "\n",
            "Epoch 4/500\n",
            "MAE after epoch 4: [44.59012236 10.22488003]\n",
            "\n",
            "Epoch 5/500\n",
            "MAE after epoch 5: [44.25564566 10.26384892]\n",
            "\n",
            "Epoch 6/500\n",
            "MAE after epoch 6: [43.75580719 10.34435228]\n",
            "\n",
            "Epoch 7/500\n",
            "MAE after epoch 7: [43.55166467 10.42435823]\n",
            "\n",
            "Epoch 8/500\n",
            "MAE after epoch 8: [42.4928696  10.61349678]\n",
            "\n",
            "Epoch 9/500\n",
            "MAE after epoch 9: [43.090744   10.56772761]\n",
            "\n",
            "Epoch 10/500\n",
            "MAE after epoch 10: [45.12046623 10.34711438]\n",
            "\n",
            "Epoch 11/500\n",
            "MAE after epoch 11: [45.65129122 10.30230228]\n",
            "\n",
            "Epoch 12/500\n",
            "MAE after epoch 12: [45.56038377 10.30603502]\n",
            "\n",
            "Epoch 13/500\n",
            "MAE after epoch 13: [45.39397685 10.31307828]\n",
            "\n",
            "Epoch 14/500\n",
            "MAE after epoch 14: [45.27833308 10.31708251]\n",
            "\n",
            "Epoch 15/500\n",
            "MAE after epoch 15: [45.2004636  10.31616693]\n",
            "\n",
            "Epoch 16/500\n",
            "MAE after epoch 16: [45.14730025 10.31238877]\n",
            "\n",
            "Epoch 17/500\n",
            "MAE after epoch 17: [45.111418   10.30904246]\n",
            "\n",
            "Epoch 18/500\n",
            "MAE after epoch 18: [45.08839579 10.30534977]\n",
            "\n",
            "Epoch 19/500\n",
            "MAE after epoch 19: [45.0733201  10.30156163]\n",
            "\n",
            "Epoch 20/500\n",
            "MAE after epoch 20: [45.0638444  10.29783722]\n",
            "\n",
            "Epoch 21/500\n",
            "MAE after epoch 21: [45.06289072 10.29396792]\n",
            "\n",
            "Epoch 22/500\n",
            "MAE after epoch 22: [45.07231302 10.28978961]\n",
            "\n",
            "Epoch 23/500\n",
            "MAE after epoch 23: [45.09205027 10.28523855]\n",
            "\n",
            "Epoch 24/500\n",
            "MAE after epoch 24: [45.1193101  10.28039171]\n",
            "\n",
            "Epoch 25/500\n",
            "MAE after epoch 25: [45.14829417 10.27551841]\n",
            "\n",
            "Epoch 26/500\n",
            "MAE after epoch 26: [45.17726298 10.27064752]\n",
            "\n",
            "Epoch 27/500\n",
            "MAE after epoch 27: [45.20953531 10.26557522]\n",
            "\n",
            "Epoch 28/500\n",
            "MAE after epoch 28: [45.25675364 10.25967929]\n",
            "\n",
            "Epoch 29/500\n",
            "MAE after epoch 29: [45.31009073 10.25354649]\n",
            "\n",
            "Epoch 30/500\n",
            "MAE after epoch 30: [45.36537333 10.24739144]\n",
            "\n",
            "Epoch 31/500\n",
            "MAE after epoch 31: [45.42270823 10.24110533]\n",
            "\n",
            "Epoch 32/500\n",
            "MAE after epoch 32: [45.48413248 10.23450359]\n",
            "\n",
            "Epoch 33/500\n",
            "MAE after epoch 33: [45.54898234 10.2311801 ]\n",
            "\n",
            "Epoch 34/500\n",
            "MAE after epoch 34: [45.61654825 10.22851739]\n",
            "\n",
            "Epoch 35/500\n",
            "MAE after epoch 35: [45.68535013 10.225806  ]\n",
            "\n",
            "Epoch 36/500\n",
            "MAE after epoch 36: [45.75193949 10.22313274]\n",
            "\n",
            "Epoch 37/500\n",
            "MAE after epoch 37: [45.81207438 10.2205916 ]\n",
            "\n",
            "Epoch 38/500\n",
            "MAE after epoch 38: [45.8656785  10.21819132]\n",
            "\n",
            "Epoch 39/500\n",
            "MAE after epoch 39: [45.91582651 10.21587807]\n",
            "\n",
            "Epoch 40/500\n",
            "MAE after epoch 40: [45.96306772 10.21365163]\n",
            "\n",
            "Epoch 41/500\n",
            "MAE after epoch 41: [46.00497599 10.21158848]\n",
            "\n",
            "Epoch 42/500\n",
            "MAE after epoch 42: [46.04036112 10.20971935]\n",
            "\n",
            "Epoch 43/500\n",
            "MAE after epoch 43: [46.06931467 10.2080115 ]\n",
            "\n",
            "Epoch 44/500\n",
            "MAE after epoch 44: [46.09373636 10.20643576]\n",
            "\n",
            "Epoch 45/500\n",
            "MAE after epoch 45: [46.11377115 10.20504193]\n",
            "\n",
            "Epoch 46/500\n",
            "MAE after epoch 46: [46.12896891 10.203923  ]\n",
            "\n",
            "Epoch 47/500\n",
            "MAE after epoch 47: [46.14097758 10.20458013]\n",
            "\n",
            "Epoch 48/500\n",
            "MAE after epoch 48: [46.14972086 10.2051498 ]\n",
            "\n",
            "Epoch 49/500\n",
            "MAE after epoch 49: [46.15512247 10.20566689]\n",
            "\n",
            "Epoch 50/500\n",
            "MAE after epoch 50: [46.15780802 10.2061519 ]\n",
            "\n",
            "Epoch 51/500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fd4b7be4a4cc>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    571\u001b[0m   \u001b[0muse_cudnn_on_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m   \u001b[0mshape_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m   \u001b[0;31m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_n\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m    733\u001b[0m   \"\"\"\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape_n\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   9442\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9443\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9444\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   9445\u001b[0m         _ctx, \"ShapeN\", name, input, \"out_type\", out_type)\n\u001b[1;32m   9446\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Apply LST"
      ],
      "metadata": {
        "id": "jBxIt6aIEDaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** LSTM"
      ],
      "metadata": {
        "id": "dRG4K4svcoHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Since LSTM expects input in the shape (batch_size, timesteps, features), our data is already suitable\n",
        "X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_lstm = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Model Creation\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X_train_lstm.shape[1], 1)))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(2))\n",
        "\n",
        "# Define optimizer and loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Custom training loop\n",
        "epochs = 500  # Maximum epochs\n",
        "batch_size = 32\n",
        "desired_mae = 3\n",
        "steps_per_epoch = len(X_train_lstm) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Create a batch of data\n",
        "        X_batch = X_train_lstm[step*batch_size:(step+1)*batch_size]\n",
        "        Y_batch = Y_train_SD[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(X_batch, training=True)\n",
        "            loss_value = loss_fn(Y_batch, predictions)\n",
        "\n",
        "        # Backward pass\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Evaluate after each epoch\n",
        "    predictions = model(X_test_lstm)\n",
        "    mae_scores = mean_absolute_error(Y_test_SD, predictions, multioutput='raw_values')\n",
        "    print(\"MAE after epoch {}: {}\".format(epoch+1, mae_scores))\n",
        "\n",
        "    # If desired MAE is achieved, stop training\n",
        "    if mae_scores[0] <= desired_mae and mae_scores[1] <= desired_mae:\n",
        "        print(\"Desired MAE achieved. Stopping training.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "MGZjvYdflHjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Data shape is already suitable for LSTM: (batch_size, timesteps, features)\n",
        "X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_lstm = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Model Creation\n",
        "model = Sequential()\n",
        "\n",
        "# Adding LSTM layer optimized for cuDNN\n",
        "model.add(LSTM(50, activation='tanh', recurrent_activation='sigmoid', input_shape=(X_train_lstm.shape[1], 1)))\n",
        "\n",
        "# Dense layers for regression\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(2))  # Two output units for S and D\n",
        "\n",
        "# Define optimizer and loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Custom training loop\n",
        "epochs = 500  # Maximum epochs\n",
        "batch_size = 32\n",
        "desired_mae = 3\n",
        "steps_per_epoch = len(X_train_lstm) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Create a batch of data\n",
        "        X_batch = X_train_lstm[step*batch_size:(step+1)*batch_size]\n",
        "        Y_batch = Y_train_SD[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(X_batch, training=True)\n",
        "            loss_value = loss_fn(Y_batch, predictions)\n",
        "\n",
        "        # Backward pass\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Evaluate after each epoch\n",
        "    predictions = model(X_test_lstm)\n",
        "    mae_scores = mean_absolute_error(Y_test_SD, predictions, multioutput='raw_values')\n",
        "    print(\"MAE after epoch {}: {}\".format(epoch+1, mae_scores))\n",
        "\n",
        "    # If desired MAE is achieved, stop training\n",
        "    if mae_scores[0] <= desired_mae and mae_scores[1] <= desired_mae:\n",
        "        print(\"Desired MAE achieved. Stopping training.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "R0pf_dFXe6VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Model Creation\n",
        "model = Sequential()\n",
        "model.add(LSTM(10, activation='tanh', recurrent_activation='sigmoid', input_shape=(X_train_lstm.shape[1], 1)))\n",
        "model.add(Dense(2))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error')\n",
        "\n",
        "# A brief summary\n",
        "model.summary()\n",
        "\n",
        "# Test with a small batch\n",
        "X_sample = X_train_lstm[:32]\n",
        "Y_sample = Y_train_SD[:32]\n",
        "model.predict(X_sample)\n"
      ],
      "metadata": {
        "id": "GrmyHSa0e9Gv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}