{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1co9L0f3cl6TV8YCyCmOdDjJmMEJKqMuR",
      "authorship_tag": "ABX9TyON1OW4IOrSzwIYePrX0GDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rumman-adnan/Assignment-Sensors-Data/blob/main/sensors_data_analytics_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4aRQ04_85VmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1784cded-318f-4395-aa9b-b0c2ede7d729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuUnVL_h8AFI",
        "outputId": "0454640d-8768-47cd-baa7-a6c4fb55d540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n"
      ],
      "metadata": {
        "id": "dzEGkJVu9NvQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "# currently dont use this\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Python work/sim_data/CSV data/training_data.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Python work/sim_data/CSV data/testing_data.csv')\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n"
      ],
      "metadata": {
        "id": "7GZWki5XERCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefe926e-b348-4e77-cb1f-07e0658e2e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 1002)\n",
            "(10000, 1002)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load('/content/drive/MyDrive/Python work/sim_data/simu_20000_0.1_90_140_train.npy')\n",
        "test_data = np.load('/content/drive/MyDrive/Python work/sim_data/simu_10000_0.1_141_178_test.npy')\n",
        "\n",
        "print(\"The shape of trained data is: \",train_data.shape)\n",
        "print(\"The shape of tested data is: \",test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy_WJs1Ujo2u",
        "outputId": "cf53db3a-c958-48d4-ca44-e4581df1d5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of trained data is:  (20000, 1006)\n",
            "The shape of tested data is:  (10000, 1006)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features and target variables from the training dataset\n",
        "train_sensor_data = train_data[:, :1000]\n",
        "train_ID = train_data[:, 1000]\n",
        "train_time = train_data[:, 1001]\n",
        "train_H = train_data[:, 1002]\n",
        "train_R = train_data[:, 1003]\n",
        "train_S = train_data[:, 1004]\n",
        "train_D = train_data[:, 1005]\n",
        "\n",
        "# Extract features and target variables from the test dataset\n",
        "test_sensor_data = test_data[:, :1000]\n",
        "test_ID = test_data[:, 1000]\n",
        "test_time = test_data[:, 1001]\n",
        "test_H = test_data[:, 1002]\n",
        "test_R = test_data[:, 1003]\n",
        "test_S = test_data[:, 1004]\n",
        "test_D = test_data[:, 1005]\n",
        "\n",
        "# Display the shape of the extracted arrays to confirm extraction\n",
        "train_sensor_data.shape, train_S.shape, train_D.shape, test_sensor_data.shape, test_S.shape, test_D.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBKNuOQQiSn9",
        "outputId": "fbcda375-cc7f-4461-81c3-4fd381ae4b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000, 1000), (20000,), (20000,), (10000, 1000), (10000,), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Reshape the data to fit the input requirements of a 1D CNN model\n",
        "# The shape should be (num_samples, num_time_steps, num_features)\n",
        "train_sensor_data_cnn = train_sensor_data.reshape(train_sensor_data.shape[0], train_sensor_data.shape[1], 1)\n",
        "test_sensor_data_cnn = test_sensor_data.reshape(test_sensor_data.shape[0], test_sensor_data.shape[1], 1)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "train_sensor_data_cnn = scaler.fit_transform(train_sensor_data_cnn.reshape(-1, 1)).reshape(train_sensor_data_cnn.shape)\n",
        "test_sensor_data_cnn = scaler.transform(test_sensor_data_cnn.reshape(-1, 1)).reshape(test_sensor_data_cnn.shape)\n",
        "\n",
        "# Show the shape of the reshaped and normalized data\n",
        "train_sensor_data_cnn.shape, test_sensor_data_cnn.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMveGehGkEM9",
        "outputId": "a708e3c4-8067-424c-ea62-5ba94ddc8f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000, 1000, 1), (10000, 1000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN model architecture\n",
        "def build_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Convolutional layers\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=256, kernel_size=5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(2))  # Two output nodes to predict both systolic and diastolic BP\n",
        "\n",
        "    # Compile the model\n",
        "    # model.compile(optimizer=Adam(), loss='mse')\n",
        "    model.compile(optimizer='adam', loss='mse', epochs = 50, verbose = True,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = build_cnn_model((train_sensor_data_cnn.shape[1], 1))\n",
        "model.fit(train_sensor_data_cnn.shape[1], epochs=2, steps_per_epoch=10)\n",
        "# Display the model architecture\n",
        "cnn_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "zyCTkncEkLE9",
        "outputId": "f353a263-2fcf-4544-ff1e-c351b29c67ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e6da499336c5>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Build the CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sensor_data_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sensor_data_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Display the model architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e6da499336c5>\u001b[0m in \u001b[0;36mbuild_cnn_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# model.compile(optimizer=Adam(), loss='mse')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     model.compile(optimizer='adam', loss='mse', epochs = 50, verbose = True,\n\u001b[0m\u001b[1;32m     25\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_validate_compile\u001b[0;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[1;32m   3595\u001b[0m         \u001b[0minvalid_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"sample_weight_mode\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3596\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minvalid_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3597\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m   3598\u001b[0m                 \u001b[0;34m\"Invalid keyword argument(s) in `compile()`: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3599\u001b[0m                 \u001b[0;34mf\"{(invalid_kwargs,)}. Valid keyword arguments include \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid keyword argument(s) in `compile()`: ({'verbose', 'epochs'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\"."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = cnn_model.predict(test_sensor_data_cnn)\n",
        "\n",
        "# Separate the predictions for systolic and diastolic BP\n",
        "pred_S, pred_D = predictions[:, 0], predictions[:, 1]\n",
        "\n",
        "# Compute the MAE for systolic and diastolic BP\n",
        "mae_S = mean_absolute_error(test_S, pred_S)\n",
        "mae_D = mean_absolute_error(test_D, pred_D)\n",
        "\n",
        "print(f\"Mean Absolute Error for Systolic BP: {mae_S}\")\n",
        "print(f\"Mean Absolute Error for Diastolic BP: {mae_D}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DEE0_OlkrYd",
        "outputId": "a631c55f-f72b-4a01-ff08-ddc8227d6b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step\n",
            "Mean Absolute Error for Systolic BP: 159.4602992392715\n",
            "Mean Absolute Error for Diastolic BP: 79.91103181238296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Apply CNN with large epochs and paramters tunning\n",
        "Read numpy arrays instead of CSV data and apply algorithms on it"
      ],
      "metadata": {
        "id": "Q14YvVD3U_ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the numpy files\n",
        "test_data = np.load('/content/drive/MyDrive/Python work/sim_data/simu_10000_0.1_141_178_test.npy')\n",
        "train_data = np.load('/content/drive/MyDrive/Python work/sim_data/simu_20000_0.1_90_140_train.npy')\n",
        "\n",
        "# Get the shape of the data arrays\n",
        "test_shape = test_data.shape\n",
        "train_shape = train_data.shape\n",
        "\n",
        "test_shape, train_shape, test_data[:3], train_data[:3]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2BPGxGcXcAm",
        "outputId": "6753a7ed-1953-407e-9aac-e9eb3f2220a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 1006),\n",
              " (20000, 1006),\n",
              " array([[-7.14960675e-08, -3.61296976e-08,  3.41059619e-07, ...,\n",
              "          1.10000000e+01,  1.72000000e+02,  8.30000000e+01],\n",
              "        [ 2.77480001e-08,  2.82283796e-08,  2.99275705e-08, ...,\n",
              "          1.30000000e+01,  1.73000000e+02,  7.90000000e+01],\n",
              "        [ 1.00426931e-07,  1.00875737e-07,  3.30280515e-07, ...,\n",
              "          2.10000000e+01,  1.46000000e+02,  7.30000000e+01]]),\n",
              " array([[-2.45845714e-07, -2.06162897e-07,  1.56348382e-06, ...,\n",
              "          1.90000000e+01,  9.10000000e+01,  9.50000000e+01],\n",
              "        [ 2.70396897e-07,  4.12740460e-07,  2.72092277e-06, ...,\n",
              "          1.60000000e+01,  1.01000000e+02,  6.10000000e+01],\n",
              "        [ 1.97552550e-07,  5.92946672e-07,  4.53845770e-06, ...,\n",
              "          1.00000000e+01,  1.31000000e+02,  8.10000000e+01]]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into features and targets\n",
        "\n",
        "# Extracting sensor data (features)\n",
        "X_train = train_data[:, :1000]\n",
        "X_test = test_data[:, :1000]\n",
        "\n",
        "# Extracting target variables for Systolic (S) and Diastolic (D) blood pressure only\n",
        "Y_train_SD = train_data[:, -2:]\n",
        "Y_test_SD = test_data[:, -2:]\n",
        "\n",
        "# Names of the target variables for reference\n",
        "target_names_SD = [\"Systolic BP\", \"Diastolic BP\"]\n",
        "\n",
        "Y_train_SD.shape, Y_test_SD.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKOykbU6YCho",
        "outputId": "ddb5f46f-040d-49c3-a9f4-43a5b7c14aa7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000, 2), (10000, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Reshaping data to be suitable for CNN (batch_size, steps, input_dim)\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Model Creation\n",
        "model = Sequential()\n",
        "\n",
        "# Adding Convolutional layers\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten and add Dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(2))  # Two output units for S and D\n",
        "\n",
        "# Compiling the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss=MeanSquaredError())\n",
        "\n",
        "# Train the model over several epochs\n",
        "epochs = 10\n",
        "history = model.fit(X_train_reshaped, Y_train_SD, validation_split=0.2, epochs=epochs, batch_size=32)\n",
        "\n",
        "# Evaluate the model's performance on the test set\n",
        "test_loss = model.evaluate(X_test_reshaped, Y_test_SD)\n",
        "\n",
        "# Predicting S and D from the test set\n",
        "predictions = model.predict(X_test_reshaped)\n",
        "\n",
        "# Calculating the MAE score\n",
        "mae_scores = mean_absolute_error(Y_test_SD, predictions, multioutput='raw_values')\n",
        "\n",
        "mae_scores\n"
      ],
      "metadata": {
        "id": "JiTclsf9kfCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Reshaping data to be suitable for CNN (batch_size, steps, input_dim)\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Model Creation\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(2))\n",
        "\n",
        "# Define optimizer and loss\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Custom training loop\n",
        "epochs = 500  # Maximum epochs\n",
        "batch_size = 32\n",
        "desired_mae = 3\n",
        "steps_per_epoch = len(X_train_reshaped) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Create a batch of data\n",
        "        X_batch = X_train_reshaped[step*batch_size:(step+1)*batch_size]\n",
        "        Y_batch = Y_train_SD[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(X_batch, training=True)\n",
        "            loss_value = loss_fn(Y_batch, predictions)\n",
        "\n",
        "        # Backward pass\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Evaluate after each epoch\n",
        "    predictions = model(X_test_reshaped)\n",
        "    mae_scores = mean_absolute_error(Y_test_SD, predictions, multioutput='raw_values')\n",
        "    print(\"MAE after epoch {}: {}\".format(epoch+1, mae_scores))\n",
        "\n",
        "    # If desired MAE is achieved, stop training\n",
        "    if mae_scores[0] <= desired_mae and mae_scores[1] <= desired_mae:\n",
        "        print(\"Desired MAE achieved. Stopping training.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "-s_U49NAQ6tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ab4e05-4488-416a-e38b-d1810d3406ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7d887f918d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7d887f918d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE after epoch 1: [41.630964   10.49081807]\n",
            "\n",
            "Epoch 2/500\n",
            "MAE after epoch 2: [43.12377963 10.26475824]\n",
            "\n",
            "Epoch 3/500\n",
            "MAE after epoch 3: [44.26942156 10.22227547]\n",
            "\n",
            "Epoch 4/500\n",
            "MAE after epoch 4: [44.59012236 10.22488003]\n",
            "\n",
            "Epoch 5/500\n",
            "MAE after epoch 5: [44.25564566 10.26384892]\n",
            "\n",
            "Epoch 6/500\n",
            "MAE after epoch 6: [43.75580719 10.34435228]\n",
            "\n",
            "Epoch 7/500\n",
            "MAE after epoch 7: [43.55166467 10.42435823]\n",
            "\n",
            "Epoch 8/500\n",
            "MAE after epoch 8: [42.4928696  10.61349678]\n",
            "\n",
            "Epoch 9/500\n",
            "MAE after epoch 9: [43.090744   10.56772761]\n",
            "\n",
            "Epoch 10/500\n",
            "MAE after epoch 10: [45.12046623 10.34711438]\n",
            "\n",
            "Epoch 11/500\n",
            "MAE after epoch 11: [45.65129122 10.30230228]\n",
            "\n",
            "Epoch 12/500\n",
            "MAE after epoch 12: [45.56038377 10.30603502]\n",
            "\n",
            "Epoch 13/500\n",
            "MAE after epoch 13: [45.39397685 10.31307828]\n",
            "\n",
            "Epoch 14/500\n",
            "MAE after epoch 14: [45.27833308 10.31708251]\n",
            "\n",
            "Epoch 15/500\n",
            "MAE after epoch 15: [45.2004636  10.31616693]\n",
            "\n",
            "Epoch 16/500\n",
            "MAE after epoch 16: [45.14730025 10.31238877]\n",
            "\n",
            "Epoch 17/500\n",
            "MAE after epoch 17: [45.111418   10.30904246]\n",
            "\n",
            "Epoch 18/500\n",
            "MAE after epoch 18: [45.08839579 10.30534977]\n",
            "\n",
            "Epoch 19/500\n",
            "MAE after epoch 19: [45.0733201  10.30156163]\n",
            "\n",
            "Epoch 20/500\n",
            "MAE after epoch 20: [45.0638444  10.29783722]\n",
            "\n",
            "Epoch 21/500\n",
            "MAE after epoch 21: [45.06289072 10.29396792]\n",
            "\n",
            "Epoch 22/500\n",
            "MAE after epoch 22: [45.07231302 10.28978961]\n",
            "\n",
            "Epoch 23/500\n",
            "MAE after epoch 23: [45.09205027 10.28523855]\n",
            "\n",
            "Epoch 24/500\n",
            "MAE after epoch 24: [45.1193101  10.28039171]\n",
            "\n",
            "Epoch 25/500\n",
            "MAE after epoch 25: [45.14829417 10.27551841]\n",
            "\n",
            "Epoch 26/500\n",
            "MAE after epoch 26: [45.17726298 10.27064752]\n",
            "\n",
            "Epoch 27/500\n",
            "MAE after epoch 27: [45.20953531 10.26557522]\n",
            "\n",
            "Epoch 28/500\n",
            "MAE after epoch 28: [45.25675364 10.25967929]\n",
            "\n",
            "Epoch 29/500\n",
            "MAE after epoch 29: [45.31009073 10.25354649]\n",
            "\n",
            "Epoch 30/500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Apply LSTM*"
      ],
      "metadata": {
        "id": "jBxIt6aIEDaw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MGZjvYdflHjQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}